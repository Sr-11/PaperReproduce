{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b598cae0-9a82-4d5c-9877-d978261d059a",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd33f101-6792-42b2-8984-f9495af87e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d449d6-accb-47f1-b8f2-5c762e7a9e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "from tqdm import trange, tqdm\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import collections\n",
    "import gzip\n",
    "import tarfile\n",
    "import tempfile\n",
    "import urllib\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c47bade6-fe44-4f10-b99a-66c9fa83238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['AE_DATA'] = '/math/home/eruisun/github/PaperReproduce/ACAI\\ 2018/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c32a928d-a70e-445d-8701-feee2d90f237",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "from lib.data import DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b833af1c-0c4b-456b-9fe3-871562175aaf",
   "metadata": {},
   "source": [
    "# Lines dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e42724f2-4a79-48f2-aa11-f9b3d7c1bc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_line(angle, height, width, w=2.):\n",
    "    m = np.zeros((height, width, 1))\n",
    "    x0 = height*0.5\n",
    "    y0 = width*0.5\n",
    "    x1 = x0 + (x0 - 1) * math.cos(-angle)\n",
    "    y1 = y0 + (y0 - 1) * math.sin(-angle)\n",
    "    flip = False\n",
    "    if abs(y0 - y1) < abs(x0 - x1):\n",
    "        x0, x1, y0, y1 = y0, y1, x0, x1\n",
    "        flip = True\n",
    "    if y1 < y0:\n",
    "        x0, x1, y0, y1 = x1, x0, y1, y0\n",
    "    x0, x1 = x0 - w / 2, x1 - w / 2\n",
    "    dx = x1 - x0\n",
    "    dy = y1 - y0\n",
    "    ds = dx / dy if dy != 0 else 0\n",
    "    yi = int(math.ceil(y0)), int(y1)\n",
    "    points = []\n",
    "    for y in range(int(y0), int(math.ceil(y1))):\n",
    "        if y < yi[0]:\n",
    "            weight = yi[0] - y0\n",
    "        elif y > yi[1]:\n",
    "            weight = y1 - yi[1]\n",
    "        else:\n",
    "            weight = 1\n",
    "        xs = x0 + (y - y0 - .5) * ds\n",
    "        xe = xs + w\n",
    "        xi = int(math.ceil(xs)), int(xe)\n",
    "        if xi[0] != xi[1]:\n",
    "            points.append((y, slice(xi[0], xi[1]), weight))\n",
    "        if xi[0] != xs:\n",
    "            points.append((y, int(xs), weight * (xi[0] - xs)))\n",
    "        if xi[1] != xe:\n",
    "            points.append((y, xi[1], weight * (xe - xi[1])))\n",
    "    if flip:\n",
    "        points = [(x, y, z) for y, x, z in points]\n",
    "    for y, x, z in points:\n",
    "        m[y, x] += 2 * z\n",
    "    m -= 1\n",
    "    m = m.clip(-1, 1)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e078bd6-f4b2-4342-b2e5-9a337bf71604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efc701bffa0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(draw_line(2, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47f928f6-a3d8-4227-acec-0a62271922c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_line(2, 32, 32).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485b53d2-d5b9-47dd-9584-7d02dd99c67c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Different autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e360cd1e-6261-4f2b-a0dd-f49ad63214d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4f18548c-04e1-4952-955d-7b2171a59cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline_encoder(tf.keras.Model):\n",
    "    def __init__(self): \n",
    "        super(Baseline_encoder, self).__init__(name='baseline_encoder')\n",
    "        self.c1 = tf.keras.layers.Conv2D(1,(3,3),padding='same',name='c1',activation=tf.keras.layers.LeakyReLU(0.2))\n",
    "        self.c2 = tf.keras.layers.Conv2D(2,(3,3),padding='same',name='c2',activation=tf.keras.layers.LeakyReLU(0.2))\n",
    "        self.ap1 = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),name='ap1')\n",
    "        \n",
    "        self.c3 = tf.keras.layers.Conv2D(2,(3,3),padding='same',name='c3',activation=tf.keras.layers.LeakyReLU(0.2))\n",
    "        self.c4 = tf.keras.layers.Conv2D(4,(3,3),padding='same',name='c4',activation=tf.keras.layers.LeakyReLU(0.2))\n",
    "        self.ap2 = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),name='ap2')\n",
    "        \n",
    "        self.c5 = tf.keras.layers.Conv2D(4,(3,3),padding='same',name='c5',activation=tf.keras.layers.LeakyReLU(0.2))\n",
    "        self.c6 = tf.keras.layers.Conv2D(8,(3,3),padding='same',name='c6',activation=tf.keras.layers.LeakyReLU(0.2))\n",
    "        self.ap3 = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),name='ap3')\n",
    "        \n",
    "        self.c7 = tf.keras.layers.Conv2D(8,(3,3),padding='same',name='c7',activation=tf.keras.layers.LeakyReLU(0.2))\n",
    "        self.c8 = tf.keras.layers.Conv2D(16,(3,3),padding='same',name='c8',activation=tf.keras.layers.LeakyReLU(0.2))\n",
    "        self.ap4 = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),name='ap4')\n",
    "\n",
    "        self.c9 = tf.keras.layers.Conv2D(16,(3,3),padding='same',name='c9',activation=tf.keras.layers.LeakyReLU(0.2))\n",
    "        self.c10 = tf.keras.layers.Conv2D(32,(3,3),padding='same',name='c10',activation=tf.keras.layers.LeakyReLU(0.2))\n",
    "        self.ap5 = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),name='ap5')\n",
    "        \n",
    "        self.c11 = tf.keras.layers.Conv2D(32,(3,3),padding='same',name='c11',activation=tf.keras.layers.LeakyReLU(0.2))\n",
    "        self.c12 = tf.keras.layers.Conv2D(64,(3,3),padding='same',name='c12')     \n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.c2(x)\n",
    "        x = self.ap1(x)\n",
    "        x = self.c3(x)\n",
    "        x = self.c4(x)\n",
    "        x = self.ap2(x)\n",
    "        x = self.c5(x)\n",
    "        x = self.c6(x)\n",
    "        x = self.ap3(x)\n",
    "        x = self.c7(x)\n",
    "        x = self.c8(x)\n",
    "        x = self.ap4(x)\n",
    "        x = self.c9(x)\n",
    "        x = self.c10(x)\n",
    "        x = self.ap5(x)\n",
    "        x = self.c11(x)\n",
    "        x = self.c12(x)\n",
    "        return x\n",
    "    \n",
    "class Baseline_decoder(tf.keras.Model):\n",
    "    def __init__(self): \n",
    "        super(Baseline_decoder, self).__init__(name='baseline_decoder')\n",
    "        leaky_relu = tf.keras.layers.LeakyReLU(0.2)\n",
    "        lecun_normal = tf.keras.initializers.LecunNormal()\n",
    "        self.c1 = tf.keras.layers.Conv2D(64,(3,3),padding='same',activation=tf.keras.layers.LeakyReLU(0.2),kernel_initializer=lecun_normal)\n",
    "        self.c2 = tf.keras.layers.Conv2D(64,(3,3),padding='same',activation=tf.keras.layers.LeakyReLU(0.2),kernel_initializer=lecun_normal)\n",
    "        self.us1 = tf.keras.layers.UpSampling2D((2, 2), interpolation='nearest')\n",
    "        \n",
    "        self.c3 = tf.keras.layers.Conv2D(32,(3,3),padding='same',activation=tf.keras.layers.LeakyReLU(0.2),kernel_initializer=lecun_normal)\n",
    "        self.c4 = tf.keras.layers.Conv2D(32,(3,3),padding='same',activation=tf.keras.layers.LeakyReLU(0.2),kernel_initializer=lecun_normal)\n",
    "        self.us2 = tf.keras.layers.UpSampling2D((2, 2), interpolation='nearest')\n",
    "        \n",
    "        self.c5 = tf.keras.layers.Conv2D(16,(3,3),padding='same',activation=tf.keras.layers.LeakyReLU(0.2),kernel_initializer=lecun_normal)\n",
    "        self.c6 = tf.keras.layers.Conv2D(16,(3,3),padding='same',activation=tf.keras.layers.LeakyReLU(0.2),kernel_initializer=lecun_normal)\n",
    "        self.us3 = tf.keras.layers.UpSampling2D((2, 2), interpolation='nearest')\n",
    "        \n",
    "        self.c7 = tf.keras.layers.Conv2D(8,(3,3),padding='same',activation=tf.keras.layers.LeakyReLU(0.2),kernel_initializer=lecun_normal)\n",
    "        self.c8 = tf.keras.layers.Conv2D(8,(3,3),padding='same',activation=tf.keras.layers.LeakyReLU(0.2),kernel_initializer=lecun_normal)\n",
    "        self.us4 = tf.keras.layers.UpSampling2D((2, 2), interpolation='nearest')\n",
    "        \n",
    "        self.c9 = tf.keras.layers.Conv2D(4,(3,3),padding='same',activation=tf.keras.layers.LeakyReLU(0.2),kernel_initializer=lecun_normal)\n",
    "        self.c10 = tf.keras.layers.Conv2D(4,(3,3),padding='same',activation=tf.keras.layers.LeakyReLU(0.2),kernel_initializer=lecun_normal)\n",
    "        self.us5 = tf.keras.layers.UpSampling2D((2, 2), interpolation='nearest')\n",
    "        \n",
    "        self.c11 = tf.keras.layers.Conv2D(2,(3,3),padding='same',activation=tf.keras.layers.LeakyReLU(0.2),kernel_initializer=lecun_normal)\n",
    "        self.c12 = tf.keras.layers.Conv2D(1,(3,3),padding='same')     \n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.c2(x)\n",
    "        x = self.us1(x)\n",
    "        x = self.c3(x)\n",
    "        x = self.c4(x)\n",
    "        x = self.us2(x)\n",
    "        x = self.c5(x)\n",
    "        x = self.c6(x)\n",
    "        x = self.us3(x)\n",
    "        x = self.c7(x)\n",
    "        x = self.c8(x)\n",
    "        x = self.us4(x)\n",
    "        x = self.c9(x)\n",
    "        x = self.c10(x)\n",
    "        x = self.us5(x)\n",
    "        x = self.c11(x)\n",
    "        x = self.c12(x)\n",
    "        return x\n",
    "\n",
    "class Baseline(tf.keras.Model):\n",
    "    def __init__(self): \n",
    "        super(Baseline, self).__init__(name='baseline')\n",
    "        self.encoder = Baseline_encoder()\n",
    "        self.decoder = Baseline_decoder()\n",
    "    def call(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)      \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab40a939-0004-41a9-8f21-3dad8dcceed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96402a4c-e565-406b-8c19-a126c9b9b805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "853052cf-5456-4b6c-b07f-6a6b257c02ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e86508d3-48ff-4ad1-84a0-476fae614b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e047701e-d2eb-4959-8a66-459132f8a05c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "584d98ab-a3ae-4dfd-94fa-50c066e013c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da60f817-221b-4cb3-88f7-26967ee24f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AEDropout(train.AE):\n",
    "\n",
    "    def model(self, latent, depth, scales, dropout):\n",
    "        x = tf.placeholder(tf.float32,\n",
    "                           [None, self.height, self.width, self.colors], 'x')\n",
    "        l = tf.placeholder(tf.float32, [None, self.nclass], 'label')\n",
    "        h = tf.placeholder(\n",
    "            tf.float32,\n",
    "            [None, self.height >> scales, self.width >> scales, latent], 'h')\n",
    "\n",
    "        encode = layers.encoder(x, scales, depth, latent, 'ae_encoder')\n",
    "        encode_train = tf.layers.flatten(encode)\n",
    "        encode_train = tf.nn.dropout(encode_train, dropout)\n",
    "        encode_train = tf.reshape(encode_train, tf.shape(encode))\n",
    "        decode = layers.decoder(h, scales, depth, self.colors, 'ae_decoder')\n",
    "        ae = layers.decoder(encode, scales, depth, self.colors, 'ae_decoder')\n",
    "        ae_train = layers.decoder(encode_train, scales, depth, self.colors,\n",
    "                                  'ae_decoder')\n",
    "        loss = tf.losses.mean_squared_error(x, ae_train)\n",
    "\n",
    "        utils.HookReport.log_tensor(loss, 'loss')\n",
    "        utils.HookReport.log_tensor(tf.sqrt(loss) * 127.5, 'rmse')\n",
    "\n",
    "        xops = classifiers.single_layer_classifier(\n",
    "            tf.stop_gradient(encode), l, self.nclass)\n",
    "        xloss = tf.reduce_mean(xops.loss)\n",
    "        utils.HookReport.log_tensor(xloss, 'classify_latent')\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = tf.train.AdamOptimizer(FLAGS.lr)\n",
    "            train_op = train_op.minimize(loss + xloss,\n",
    "                                         tf.train.get_global_step())\n",
    "        ops = train.AEOps(x, h, l, encode, decode, ae, train_op,\n",
    "                          classify_latent=xops.output)\n",
    "\n",
    "        def gen_images():\n",
    "            return self.make_sample_grid_and_save(ops)\n",
    "\n",
    "        recon, inter, slerp, samples = tf.py_func(\n",
    "            gen_images, [], [tf.float32]*4)\n",
    "        tf.summary.image('reconstruction', tf.expand_dims(recon, 0))\n",
    "        tf.summary.image('interpolation', tf.expand_dims(inter, 0))\n",
    "        tf.summary.image('slerp', tf.expand_dims(slerp, 0))\n",
    "        tf.summary.image('samples', tf.expand_dims(samples, 0))\n",
    "\n",
    "        return ops\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6a5acb-6319-45f6-ab64-4895d5784a14",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aed2d5f-05d4-4a71-9216-6b6832ea7712",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AEDenoising(train.AE):\n",
    "\n",
    "    def model(self, latent, depth, scales, noise):\n",
    "        x = tf.placeholder(tf.float32,\n",
    "                           [None, self.height, self.width, self.colors], 'x')\n",
    "        l = tf.placeholder(tf.float32, [None, self.nclass], 'label')\n",
    "        h = tf.placeholder(\n",
    "            tf.float32,\n",
    "            [None, self.height >> scales, self.width >> scales, latent], 'h')\n",
    "\n",
    "        xn = x + tf.random_normal(tf.shape(x), stddev=noise)\n",
    "        encode = layers.encoder(x, scales, depth, latent, 'ae_encoder')\n",
    "        decode = layers.decoder(h, scales, depth, self.colors, 'ae_decoder')\n",
    "        ae = layers.decoder(encode, scales, depth, self.colors, 'ae_decoder')\n",
    "        encode_noise = layers.encoder(xn, scales, depth, latent, 'ae_encoder')\n",
    "        ae_noise = layers.decoder(encode_noise, scales, depth, self.colors,\n",
    "                                  'ae_decoder')\n",
    "        loss = tf.losses.mean_squared_error(x, ae_noise)\n",
    "\n",
    "        utils.HookReport.log_tensor(loss, 'loss')\n",
    "        utils.HookReport.log_tensor(tf.sqrt(loss) * 127.5, 'rmse')\n",
    "\n",
    "        xops = classifiers.single_layer_classifier(\n",
    "            tf.stop_gradient(encode), l, self.nclass)\n",
    "        xloss = tf.reduce_mean(xops.loss)\n",
    "        utils.HookReport.log_tensor(xloss, 'classify_latent')\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = tf.train.AdamOptimizer(FLAGS.lr)\n",
    "            train_op = train_op.minimize(loss + xloss,\n",
    "                                         tf.train.get_global_step())\n",
    "        ops = train.AEOps(x, h, l, encode, decode, ae, train_op,\n",
    "                          classify_latent=xops.output)\n",
    "\n",
    "        def gen_images():\n",
    "            return self.make_sample_grid_and_save(ops)\n",
    "\n",
    "        recon, inter, slerp, samples = tf.py_func(\n",
    "            gen_images, [], [tf.float32]*4)\n",
    "        tf.summary.image('reconstruction', tf.expand_dims(recon, 0))\n",
    "        tf.summary.image('interpolation', tf.expand_dims(inter, 0))\n",
    "        tf.summary.image('slerp', tf.expand_dims(slerp, 0))\n",
    "        tf.summary.image('samples', tf.expand_dims(samples, 0))\n",
    "\n",
    "        return ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4594b294-f167-400e-bb55-93ccfc72ad01",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d3b64e-ad54-49dc-aa47-603b782e6629",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(train.AE):\n",
    "\n",
    "    def model(self, latent, depth, scales, beta):\n",
    "        x = tf.placeholder(tf.float32,\n",
    "                           [None, self.height, self.width, self.colors], 'x')\n",
    "        l = tf.placeholder(tf.float32, [None, self.nclass], 'label')\n",
    "        h = tf.placeholder(\n",
    "            tf.float32,\n",
    "            [None, self.height >> scales, self.width >> scales, latent], 'h')\n",
    "\n",
    "        def encoder(x):\n",
    "            return layers.encoder(x, scales, depth, latent, 'ae_enc')\n",
    "\n",
    "        def decoder(h):\n",
    "            return layers.decoder(h, scales, depth, self.colors, 'ae_dec')\n",
    "\n",
    "        encode = encoder(x)\n",
    "        with tf.variable_scope('ae_latent'):\n",
    "            encode_shape = tf.shape(encode)\n",
    "            encode_flat = tf.layers.flatten(encode)\n",
    "            latent_dim = encode_flat.get_shape()[-1]\n",
    "            q_mu = tf.layers.dense(encode_flat, latent_dim)\n",
    "            log_q_sigma_sq = tf.layers.dense(encode_flat, latent_dim)\n",
    "        q_sigma = tf.sqrt(tf.exp(log_q_sigma_sq))\n",
    "        q_z = tf.distributions.Normal(loc=q_mu, scale=q_sigma)\n",
    "        q_z_sample = q_z.sample()\n",
    "        q_z_sample_reshaped = tf.reshape(q_z_sample, encode_shape)\n",
    "        p_x_given_z_logits = decoder(q_z_sample_reshaped)\n",
    "        p_x_given_z = tf.distributions.Bernoulli(logits=p_x_given_z_logits)\n",
    "        ae = 2*tf.nn.sigmoid(p_x_given_z_logits) - 1\n",
    "        decode = 2*tf.nn.sigmoid(decoder(h)) - 1\n",
    "        loss_kl = 0.5*tf.reduce_sum(\n",
    "            -log_q_sigma_sq - 1 + tf.exp(log_q_sigma_sq) + q_mu**2)\n",
    "        loss_kl = loss_kl/tf.to_float(tf.shape(x)[0])\n",
    "        x_bernoulli = 0.5*(x + 1)\n",
    "        loss_ll = tf.reduce_sum(p_x_given_z.log_prob(x_bernoulli))\n",
    "        loss_ll = loss_ll/tf.to_float(tf.shape(x)[0])\n",
    "        elbo = loss_ll - beta*loss_kl\n",
    "\n",
    "        utils.HookReport.log_tensor(loss_kl, 'loss_kl')\n",
    "        utils.HookReport.log_tensor(loss_ll, 'loss_ll')\n",
    "        utils.HookReport.log_tensor(elbo, 'elbo')\n",
    "\n",
    "        xops = classifiers.single_layer_classifier(\n",
    "            tf.stop_gradient(encode), l, self.nclass)\n",
    "        xloss = tf.reduce_mean(xops.loss)\n",
    "        utils.HookReport.log_tensor(xloss, 'classify_latent')\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        ae_vars = tf.global_variables('ae_')\n",
    "        xl_vars = tf.global_variables('single_layer_classifier')\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_ae = tf.train.AdamOptimizer(FLAGS.lr).minimize(\n",
    "                -elbo, var_list=ae_vars)\n",
    "            train_xl = tf.train.AdamOptimizer(FLAGS.lr).minimize(\n",
    "                xloss, tf.train.get_global_step(), var_list=xl_vars)\n",
    "        ops = train.AEOps(x, h, l, q_z_sample_reshaped, decode, ae,\n",
    "                          tf.group(train_ae, train_xl),\n",
    "                          classify_latent=xops.output)\n",
    "\n",
    "        n_interpolations = 16\n",
    "        n_images_per_interpolation = 16\n",
    "\n",
    "        def gen_images():\n",
    "            return self.make_sample_grid_and_save(\n",
    "                ops, interpolation=n_interpolations,\n",
    "                height=n_images_per_interpolation)\n",
    "\n",
    "        recon, inter, slerp, samples = tf.py_func(\n",
    "            gen_images, [], [tf.float32]*4)\n",
    "        tf.summary.image('reconstruction', tf.expand_dims(recon, 0))\n",
    "        tf.summary.image('interpolation', tf.expand_dims(inter, 0))\n",
    "        tf.summary.image('slerp', tf.expand_dims(slerp, 0))\n",
    "        tf.summary.image('samples', tf.expand_dims(samples, 0))\n",
    "\n",
    "        return ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab7a9d2-f955-4064-ab47-39bd2a2e1abd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### AAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a5eb6e-9596-4e59-9507-b3dc3a186dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAE(train.AE):\n",
    "\n",
    "    def model(self, latent, depth, scales, adversary_lr, disc_layer_sizes):\n",
    "        x = tf.placeholder(tf.float32,\n",
    "                           [None, self.height, self.width, self.colors], 'x')\n",
    "        l = tf.placeholder(tf.float32, [None, self.nclass], 'label')\n",
    "        h = tf.placeholder(\n",
    "            tf.float32,\n",
    "            [None, self.height >> scales, self.width >> scales, latent], 'h')\n",
    "\n",
    "        def encoder(x):\n",
    "            return layers.encoder(x, scales, depth, latent, 'ae_enc')\n",
    "\n",
    "        def decoder(h):\n",
    "            return layers.decoder(h, scales, depth, self.colors, 'ae_dec')\n",
    "\n",
    "        def discriminator(h):\n",
    "            with tf.variable_scope('disc', reuse=tf.AUTO_REUSE):\n",
    "                h = tf.layers.flatten(h)\n",
    "                for size in [int(s) for s in disc_layer_sizes.split(',')]:\n",
    "                    h = tf.layers.dense(h, size, tf.nn.leaky_relu)\n",
    "                return tf.layers.dense(h, 1)\n",
    "\n",
    "        encode = encoder(x)\n",
    "        decode = decoder(h)\n",
    "        ae = decoder(encode)\n",
    "        loss_ae = tf.losses.mean_squared_error(x, ae)\n",
    "\n",
    "        prior_samples = tf.random_normal(tf.shape(encode), dtype=encode.dtype)\n",
    "        adversary_logit_latent = discriminator(encode)\n",
    "        adversary_logit_prior = discriminator(prior_samples)\n",
    "        adversary_loss_latents = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits=adversary_logit_latent,\n",
    "                labels=tf.zeros_like(adversary_logit_latent)))\n",
    "        adversary_loss_prior = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits=adversary_logit_prior,\n",
    "                labels=tf.ones_like(adversary_logit_prior)))\n",
    "        autoencoder_loss_latents = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits=adversary_logit_latent,\n",
    "                labels=tf.ones_like(adversary_logit_latent)))\n",
    "\n",
    "        def _accuracy(logits, label):\n",
    "            labels = tf.logical_and(label, tf.ones_like(logits, dtype=bool))\n",
    "            correct = tf.equal(tf.greater(logits, 0), labels)\n",
    "            return tf.reduce_mean(tf.to_float(correct))\n",
    "        latent_accuracy = _accuracy(adversary_logit_latent, False)\n",
    "        prior_accuracy = _accuracy(adversary_logit_prior, True)\n",
    "        adversary_accuracy = (latent_accuracy + prior_accuracy)/2\n",
    "\n",
    "        utils.HookReport.log_tensor(loss_ae, 'loss_ae')\n",
    "        utils.HookReport.log_tensor(adversary_loss_latents, 'loss_adv_latent')\n",
    "        utils.HookReport.log_tensor(adversary_loss_prior, 'loss_adv_prior')\n",
    "        utils.HookReport.log_tensor(autoencoder_loss_latents, 'loss_ae_latent')\n",
    "        utils.HookReport.log_tensor(adversary_accuracy, 'adversary_accuracy')\n",
    "\n",
    "        xops = classifiers.single_layer_classifier(\n",
    "            tf.stop_gradient(encode), l, self.nclass)\n",
    "        xloss = tf.reduce_mean(xops.loss)\n",
    "        utils.HookReport.log_tensor(xloss, 'classify_latent')\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        ae_vars = tf.global_variables('ae_')\n",
    "        disc_vars = tf.global_variables('disc')\n",
    "        xl_vars = tf.global_variables('single_layer_classifier')\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_ae = tf.train.AdamOptimizer(FLAGS.lr).minimize(\n",
    "                loss_ae + autoencoder_loss_latents, var_list=ae_vars)\n",
    "            train_disc = tf.train.AdamOptimizer(adversary_lr).minimize(\n",
    "                adversary_loss_prior + adversary_loss_latents,\n",
    "                var_list=disc_vars)\n",
    "            train_xl = tf.train.AdamOptimizer(FLAGS.lr).minimize(\n",
    "                xloss, tf.train.get_global_step(), var_list=xl_vars)\n",
    "        ops = train.AEOps(x, h, l, encode, decode, ae,\n",
    "                          tf.group(train_ae, train_disc, train_xl),\n",
    "                          classify_latent=xops.output)\n",
    "\n",
    "        n_interpolations = 16\n",
    "        n_images_per_interpolation = 16\n",
    "\n",
    "        def gen_images():\n",
    "            return self.make_sample_grid_and_save(\n",
    "                ops, interpolation=n_interpolations,\n",
    "                height=n_images_per_interpolation)\n",
    "\n",
    "        recon, inter, slerp, samples = tf.py_func(\n",
    "            gen_images, [], [tf.float32]*4)\n",
    "        tf.summary.image('reconstruction', tf.expand_dims(recon, 0))\n",
    "        tf.summary.image('interpolation', tf.expand_dims(inter, 0))\n",
    "        tf.summary.image('slerp', tf.expand_dims(slerp, 0))\n",
    "        tf.summary.image('samples', tf.expand_dims(samples, 0))\n",
    "\n",
    "        if FLAGS.dataset == 'lines32':\n",
    "            batched = (n_interpolations, 32, n_images_per_interpolation, 32, 1)\n",
    "            batched_interp = tf.transpose(\n",
    "                tf.reshape(inter, batched), [0, 2, 1, 3, 4])\n",
    "            mean_distance, mean_smoothness = tf.py_func(\n",
    "                eval.line_eval, [batched_interp], [tf.float32, tf.float32])\n",
    "            tf.summary.scalar('mean_distance', mean_distance)\n",
    "            tf.summary.scalar('mean_smoothness', mean_smoothness)\n",
    "\n",
    "        return ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb2360b-0e9f-4592-9b23-ccba8d1dbe58",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### VQ-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730fa9ba-151f-4a52-b0ca-da8db19c9669",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AEVQVAE(train.AE):\n",
    "    hparams = ClassDict(decay=0.999,\n",
    "                        random_top_k=1,\n",
    "                        filter_size=512,\n",
    "                        soft_em=False,\n",
    "                        num_samples=1,\n",
    "                        epsilon=1e-5)\n",
    "\n",
    "    def model(self, latent, depth, scales, z_log_size, beta, num_latents):\n",
    "        tf.set_random_seed(123)\n",
    "        x = tf.placeholder(tf.float32,\n",
    "                           [None, self.height, self.width, self.colors], 'x')\n",
    "        l = tf.placeholder(tf.float32, [None, self.nclass], 'label')\n",
    "        h = tf.placeholder(\n",
    "            tf.float32,\n",
    "            [None, self.height >> scales, self.width >> scales, latent], 'h')\n",
    "\n",
    "        def decode_fn(h):\n",
    "            with tf.variable_scope('vqvae', reuse=tf.AUTO_REUSE):\n",
    "                h2 = tf.expand_dims(tf.layers.flatten(h), axis=1)\n",
    "                h2 = tf.layers.dense(h2, self.hparams.hidden_size * num_latents)\n",
    "                d = bneck.discrete_bottleneck(h2)\n",
    "                y = layers.decoder(tf.reshape(d['dense'], tf.shape(h)),\n",
    "                                   scales, depth, self.colors, 'ae_decoder')\n",
    "                return y, d\n",
    "\n",
    "        self.hparams.hidden_size = (\n",
    "                    (self.height >> scales) * (self.width >> scales) * latent)\n",
    "        self.hparams.z_size = z_log_size\n",
    "        self.hparams.num_residuals = 1\n",
    "        self.hparams.num_blocks = 1\n",
    "        self.hparams.beta = beta\n",
    "        self.hparams.ema = True\n",
    "        bneck = DiscreteBottleneck(self.hparams)\n",
    "        encode = layers.encoder(x, scales, depth, latent, 'ae_encoder')\n",
    "        decode = decode_fn(h)[0]\n",
    "        ae, d = decode_fn(encode)\n",
    "        loss_ae = tf.losses.mean_squared_error(x, ae)\n",
    "\n",
    "        utils.HookReport.log_tensor(tf.sqrt(loss_ae) * 127.5, 'rmse')\n",
    "        utils.HookReport.log_tensor(loss_ae, 'loss_ae')\n",
    "        utils.HookReport.log_tensor(d['loss'], 'vqvae_loss')\n",
    "\n",
    "        xops = classifiers.single_layer_classifier(\n",
    "            tf.stop_gradient(d['dense']), l, self.nclass)\n",
    "        xloss = tf.reduce_mean(xops.loss)\n",
    "        utils.HookReport.log_tensor(xloss, 'classify_latent')\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops + [d['discrete']]):\n",
    "            train_op = tf.train.AdamOptimizer(FLAGS.lr).minimize(\n",
    "                loss_ae + xloss + d['loss'], tf.train.get_global_step())\n",
    "        ops = train.AEOps(x, h, l, encode, decode, ae, train_op,\n",
    "                          classify_latent=xops.output)\n",
    "\n",
    "        n_interpolations = 16\n",
    "        n_images_per_interpolation = 16\n",
    "\n",
    "        def gen_images():\n",
    "            return self.make_sample_grid_and_save(\n",
    "                ops, interpolation=n_interpolations,\n",
    "                height=n_images_per_interpolation)\n",
    "\n",
    "        recon, inter, slerp, samples = tf.py_func(\n",
    "            gen_images, [], [tf.float32]*4)\n",
    "        tf.summary.image('reconstruction', tf.expand_dims(recon, 0))\n",
    "        tf.summary.image('interpolation', tf.expand_dims(inter, 0))\n",
    "        tf.summary.image('slerp', tf.expand_dims(slerp, 0))\n",
    "        tf.summary.image('samples', tf.expand_dims(samples, 0))\n",
    "\n",
    "        return ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b90f637-bd72-419b-b218-4ef1606c0e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35e5dd5a-18fa-476e-b7c7-bca2198a8dd4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ACAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a36f79-37c4-4073-b618-4f1030f68d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69a2f4a-d8de-4bdd-bfc0-06d1dbb39aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b86c26-4108-4bf7-b7f1-ab7096c75b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACAI(train.AE):\n",
    "    def model(self, latent, depth, scales, advweight, advdepth, reg):\n",
    "        x = tf.placeholder(tf.float32,\n",
    "                           [None, self.height, self.width, self.colors], 'x')\n",
    "        l = tf.placeholder(tf.float32, [None, self.nclass], 'label')\n",
    "        h = tf.placeholder(\n",
    "            tf.float32,\n",
    "            [None, self.height >> scales, self.width >> scales, latent], 'h')\n",
    "\n",
    "        def encoder(x):\n",
    "            return layers.encoder(x, scales, depth, latent, 'ae_enc')\n",
    "\n",
    "        def decoder(h):\n",
    "            v = layers.decoder(h, scales, depth, self.colors, 'ae_dec')\n",
    "            return v\n",
    "\n",
    "        def disc(x):\n",
    "            return tf.reduce_mean(\n",
    "                layers.encoder(x, scales, advdepth, latent, 'disc'),\n",
    "                axis=[1, 2, 3])\n",
    "\n",
    "        encode = encoder(x)\n",
    "        decode = decoder(h)\n",
    "        ae = decoder(encode)\n",
    "        loss_ae = tf.losses.mean_squared_error(x, ae)\n",
    "\n",
    "        alpha = tf.random_uniform([tf.shape(encode)[0], 1, 1, 1], 0, 1)\n",
    "        alpha = 0.5 - tf.abs(alpha - 0.5)  # Make interval [0, 0.5]\n",
    "        encode_mix = alpha * encode + (1 - alpha) * encode[::-1]\n",
    "        decode_mix = decoder(encode_mix)\n",
    "\n",
    "        loss_disc = tf.reduce_mean(\n",
    "            tf.square(disc(decode_mix) - alpha[:, 0, 0, 0]))\n",
    "        loss_disc_real = tf.reduce_mean(tf.square(disc(ae + reg * (x - ae))))\n",
    "        loss_ae_disc = tf.reduce_mean(tf.square(disc(decode_mix)))\n",
    "\n",
    "        utils.HookReport.log_tensor(tf.sqrt(loss_ae) * 127.5, 'rmse')\n",
    "        utils.HookReport.log_tensor(loss_ae, 'loss_ae')\n",
    "        utils.HookReport.log_tensor(loss_disc, 'loss_disc')\n",
    "        utils.HookReport.log_tensor(loss_ae_disc, 'loss_ae_disc')\n",
    "        utils.HookReport.log_tensor(loss_disc_real, 'loss_disc_real')\n",
    "\n",
    "        xops = classifiers.single_layer_classifier(\n",
    "            tf.stop_gradient(encode), l, self.nclass)\n",
    "        xloss = tf.reduce_mean(xops.loss)\n",
    "        utils.HookReport.log_tensor(xloss, 'classify_latent')\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        ae_vars = tf.global_variables('ae_')\n",
    "        disc_vars = tf.global_variables('disc')\n",
    "        xl_vars = tf.global_variables('single_layer_classifier')\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_ae = tf.train.AdamOptimizer(FLAGS.lr).minimize(\n",
    "                loss_ae + advweight * loss_ae_disc,\n",
    "                var_list=ae_vars)\n",
    "            train_d = tf.train.AdamOptimizer(FLAGS.lr).minimize(\n",
    "                loss_disc + loss_disc_real,\n",
    "                var_list=disc_vars)\n",
    "            train_xl = tf.train.AdamOptimizer(FLAGS.lr).minimize(\n",
    "                xloss, tf.train.get_global_step(), var_list=xl_vars)\n",
    "        ops = train.AEOps(x, h, l, encode, decode, ae,\n",
    "                          tf.group(train_ae, train_d, train_xl),\n",
    "                          classify_latent=xops.output)\n",
    "\n",
    "        n_interpolations = 16\n",
    "        n_images_per_interpolation = 16\n",
    "\n",
    "        def gen_images():\n",
    "            return self.make_sample_grid_and_save(\n",
    "                ops, interpolation=n_interpolations,\n",
    "                height=n_images_per_interpolation)\n",
    "\n",
    "        recon, inter, slerp, samples = tf.py_func(\n",
    "            gen_images, [], [tf.float32] * 4)\n",
    "        tf.summary.image('reconstruction', tf.expand_dims(recon, 0))\n",
    "        tf.summary.image('interpolation', tf.expand_dims(inter, 0))\n",
    "        tf.summary.image('slerp', tf.expand_dims(slerp, 0))\n",
    "        tf.summary.image('samples', tf.expand_dims(samples, 0))\n",
    "\n",
    "        if FLAGS.dataset == 'lines32':\n",
    "            batched = (n_interpolations, 32, n_images_per_interpolation, 32, 1)\n",
    "            batched_interp = tf.transpose(\n",
    "                tf.reshape(inter, batched), [0, 2, 1, 3, 4])\n",
    "            mean_distance, mean_smoothness = tf.py_func(\n",
    "                eval.line_eval, [batched_interp], [tf.float32, tf.float32])\n",
    "            tf.summary.scalar('mean_distance', mean_distance)\n",
    "            tf.summary.scalar('mean_smoothness', mean_smoothness)\n",
    "\n",
    "        return ops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32745b96-0c29-4bf9-9fe2-d126b013f57c",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cf9cae-90c8-4b17-9239-abe3863452c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Baseline()\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        clear_output(wait=True)\n",
    "        print(\"For epoch {:d}, loss is {:f}.\".format(epoch, logs[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da089306-1af5-4e04-8033-b35d8cf1cc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2^24 is too large for our gpu\n",
    "# use 2^12\n",
    "dataset = np.empty((2**12,32,32,1))\n",
    "for i in range(2**12):\n",
    "    angle = 2 * math.pi * np.random.random()\n",
    "    dataset[i,:,:,:] = draw_line(angle, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e331a94d-fee9-4b38-848e-2994fed49362",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adam = tf.keras.optimizers.Adam(learning_rate=0.1,beta_1=0.9,beta_2=0.999,epsilon=1e-08)\n",
    "model.compile(loss='mse',optimizer=Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87ca50f-7278-4d3a-ba66-363c4bbe19da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 25, loss is 0.080612.\n"
     ]
    }
   ],
   "source": [
    "model.fit(dataset, dataset, batch_size=64,epochs=1000,verbose=0,callbacks=CustomCallback())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "46f6e27f-bd8d-4fdf-8bec-2ecebd3e379c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ef8501fbfd0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(model(draw_line(2*math.pi*np.random.random(), 32, 32)[np.newaxis,:,:,:])[0,:,:,:])\n",
    "plt.imshow(model(dataset[0:1,:,:,:])[0,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "02fc4fbe-621f-408f-b672-bd78bc52d0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ef8500b4af0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAMuklEQVR4nO3db4wc9X3H8fe3zmGHP1XsEpBlUAjGD4pQMOjk0rqKaCngICSgEiQ8SPwA9VIJEKhpK4tKDX1G/0BSRQqqKRZuRSFIgCAthVgWFYqUEAw1/lOnwSCHOLbspCaCpqox8O2DHUtn9/Zub3d29+D7fkmrnfn9Zu731eg+N7MzezORmUj66PuVcRcgaTQMu1SEYZeKMOxSEYZdKsKwS0V8bJCVI2Id8LfAIuDvM/Oe2ZY/JRbnEk4bZEhJs/hffsm7eTRm6ot+r7NHxCLgR8CVwH7gJeDmzPyPbuv8aizL34gr+hpP0txezK28nUdmDPsgh/FrgL2Z+UZmvgs8Clw3wM+TNESDhH0F8JNp8/ubNkkL0CCf2Wc6VPh/nwkiYgqYAljCqQMMJ2kQg+zZ9wPnTps/Bzhw8kKZuTEzJzNzcoLFAwwnaRCDhP0lYFVEfDoiTgG+ADzdTlmS2tb3YXxmvhcRtwHP0bn0tikzd7dWmaRWDXSdPTOfAZ5pqRZJQ+Q36KQiDLtUhGGXijDsUhGGXSrCsEtFGHapCMMuFWHYpSIMu1SEYZeKMOxSEYZdKsKwS0UYdqkIwy4VYdilIgy7VIRhl4ow7FIRhl0qwrBLRRh2qQjDLhVh2KUiBnoiTETsA94B3gfey8zJNoqS1L6Bwt74ncz8eQs/R9IQeRgvFTFo2BP4TkS8HBFTbRQkaTgGPYxfm5kHIuIsYEtE/DAzX5i+QPNHYApgCacOOJykfg20Z8/MA837YeBJYM0My2zMzMnMnJxg8SDDSRpA32GPiNMi4ozj08BVwK62CpPUrkEO488GnoyI4z/nnzLz2VaqktS6vsOemW8AF7dYi6Qh8tKbVIRhl4ow7FIRhl0qwrBLRRh2qQjDLhVh2KUiDLtUhGGXijDsUhGGXSrCsEtFGHapCMMuFWHYpSIMu1SEYZeKMOxSEYZdKsKwS0UYdqkIwy4VYdilIgy7VMScT4SJiE3AtcDhzLyoaVsGfAs4D9gH3JSZbw2vTOmjK9eunrF9701Luq6z6o7vz3ucXvbsDwHrTmrbAGzNzFXA1mZe0gI2Z9ib560fOan5OmBzM70ZuL7dsiS1rd/P7Gdn5kGA5v2s9kqSNAyDPLK5JxExBUwBLOHUYQ8nqYt+9+yHImI5QPN+uNuCmbkxMyczc3KCxX0OJ2lQ/Yb9aWB9M70eeKqdciQNSy+X3h4BLgfOjIj9wFeBe4DHIuIW4E3gxmEWKS0U+VsXd+3b+/mPd+370yu/3bXvDz/x0LzruPqO1fNeZ86wZ+bNXbqumPdoksbGb9BJRRh2qQjDLhVh2KUiDLtUxNC/QSeNU7dLZa/f1P0y2Z9cNdtlss1d+/r110dWztj+d89e2XWdlQznv94kfQQYdqkIwy4VYdilIgy7VIRhl4rw0psWjPzN7v9RNtulsj+6+l+69t3a8qWy+46c37Xvm89e1bXvgm/9smtfvrRzxvZ+Lq/Nxj27VIRhl4ow7FIRhl0qwrBLRXg2Xv277DNdu/Z+fubbht951b92Xef2pe3/k0m3s+fffG6WM+ePzv/MOcx+9jy79oyOe3apCMMuFWHYpSIMu1SEYZeKMOxSEb08/mkTcC1wODMvatruBv4A+Fmz2F2Z+cywitT4XLv7ra59ty/9h1bH+vpb53Xt+8Zz67r2zXapjB/M/59MFsJlsmHoZc/+EDDTlv5aZq5uXgZdWuDmDHtmvgAcGUEtkoZokM/st0XEjojYFBFLW6tI0lD0G/b7gZXAauAgcG+3BSNiKiK2RcS2YxztczhJg+or7Jl5KDPfz8wPgAeANbMsuzEzJzNzcoLF/dYpaUB9hT0ilk+bvQHY1U45koall0tvjwCXA2dGxH7gq8DlEbGazlWKfcCXh1eixun2pT/u2veNtz7Vte/rz31uxvYLHvuf7oN9f0fXrgtavh9bRXOGPTNvnqH5wSHUImmI/AadVIRhl4ow7FIRhl0qwrBLRXjDSc3q6t//UvdOL5V9qLhnl4ow7FIRhl0qwrBLRRh2qQjDLhXhpTfNbpbLa/pwcc8uFWHYpSIMu1SEYZeKMOxSEYZdKsKwS0UYdqkIwy4VYdilIgy7VIRhl4qYM+wRcW5EPB8ReyJid0Tc0bQvi4gtEfFa8+5jm6UFrJc9+3vAVzLz14HLgFsj4kJgA7A1M1cBW5t5SQvUnGHPzIOZ+Uoz/Q6wB1gBXAdsbhbbDFw/pBoltWBen9kj4jzgEuBF4OzMPAidPwjAWa1XJ6k1PYc9Ik4HHgfuzMy357HeVERsi4htxzjaT42SWtBT2CNigk7QH87MJ5rmQxGxvOlfDhyead3M3JiZk5k5OcHiNmqW1IdezsYHneex78nM+6Z1PQ2sb6bXA0+1X56ktvRyD7q1wBeBnRGxvWm7C7gHeCwibgHeBG4cSoWSWjFn2DPzu0B06b6i3XIkDYvfoJOKMOxSEYZdKsKwS0UYdqkIwy4VYdilIgy7VIRhl4ow7FIRhl0qwrBLRRh2qQjDLhVh2KUiDLtUhGGXijDsUhGGXSrCsEtFGHapCMMuFWHYpSIMu1SEYZeK6OVZb+dGxPMRsScidkfEHU373RHx04jY3ryuGX65kvrVy7Pe3gO+kpmvRMQZwMsRsaXp+1pm/s3wypPUll6e9XYQONhMvxMRe4AVwy5MUrvm9Zk9Is4DLgFebJpui4gdEbEpIpa2XZyk9vQc9og4HXgcuDMz3wbuB1YCq+ns+e/tst5URGyLiG3HODp4xZL60lPYI2KCTtAfzswnADLzUGa+n5kfAA8Aa2ZaNzM3ZuZkZk5OsLituiXNUy9n4wN4ENiTmfdNa18+bbEbgF3tlyepLb2cjV8LfBHYGRHbm7a7gJsjYjWQwD7gy0OoT1JLejkb/10gZuh6pv1yJA2L36CTijDsUhGGXSrCsEtFGHapCMMuFWHYpSIMu1SEYZeKMOxSEYZdKsKwS0UYdqkIwy4VYdilIgy7VIRhl4ow7FIRhl0qwrBLRRh2qQjDLhVh2KUiDLtUhGGXiujlWW9LIuIHEfFqROyOiL9o2pdFxJaIeK1595HN0gLWy579KPC7mXkxncczr4uIy4ANwNbMXAVsbeYlLVBzhj07/ruZnWheCVwHbG7aNwPXD6NASe3o9fnsi5onuB4GtmTmi8DZmXkQoHk/a2hVShpYT2HPzPczczVwDrAmIi7qdYCImIqIbRGx7RhH+yxT0qDmdTY+M38B/BuwDjgUEcsBmvfDXdbZmJmTmTk5weLBqpXUt17Oxn8yIj7RTH8c+D3gh8DTwPpmsfXAU0OqUVILPtbDMsuBzRGxiM4fh8cy858j4nvAYxFxC/AmcOMQ65Q0oDnDnpk7gEtmaP8v4IphFCWpfX6DTirCsEtFGHapCMMuFWHYpSIiM0c3WMTPgB83s2cCPx/Z4N1Zx4ms40Qftjo+lZmfnKljpGE/YeCIbZk5OZbBrcM6CtbhYbxUhGGXihhn2DeOcezprONE1nGij0wdY/vMLmm0PIyXihhL2CNiXUT8Z0TsjYix3bsuIvZFxM6I2B4R20Y47qaIOBwRu6a1jfwGnl3quDsiftpsk+0Rcc0I6jg3Ip6PiD3NTU3vaNpHuk1mqWOk22RoN3nNzJG+gEXA68D5wCnAq8CFo66jqWUfcOYYxv0scCmwa1rbXwEbmukNwF+OqY67gT8e8fZYDlzaTJ8B/Ai4cNTbZJY6RrpNgABOb6YngBeBywbdHuPYs68B9mbmG5n5LvAonZtXlpGZLwBHTmoe+Q08u9Qxcpl5MDNfaabfAfYAKxjxNpmljpHKjtZv8jqOsK8AfjJtfj9j2KCNBL4TES9HxNSYajhuId3A87aI2NEc5o/0eQARcR6d+yeM9aamJ9UBI94mw7jJ6zjCHjO0jeuSwNrMvBT4HHBrRHx2THUsJPcDK+k8I+AgcO+oBo6I04HHgTsz8+1RjdtDHSPfJjnATV67GUfY9wPnTps/BzgwhjrIzAPN+2HgSTofMcalpxt4DltmHmp+0T4AHmBE2yQiJugE7OHMfKJpHvk2mamOcW2TZuxfMM+bvHYzjrC/BKyKiE9HxCnAF+jcvHKkIuK0iDjj+DRwFbBr9rWGakHcwPP4L1PjBkawTSIigAeBPZl537SukW6TbnWMepsM7SavozrDeNLZxmvonOl8HfizMdVwPp0rAa8Cu0dZB/AIncPBY3SOdG4Bfo3OY7Rea96XjamOfwR2AjuaX67lI6jjt+l8lNsBbG9e14x6m8xSx0i3CfAZ4N+b8XYBf960D7Q9/AadVITfoJOKMOxSEYZdKsKwS0UYdqkIwy4VYdilIgy7VMT/AUzjSW34bPjyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dataset[0,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6f9ab1-0ccc-46bb-855d-e20d5ea22aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
